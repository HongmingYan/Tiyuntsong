# Tiyuntsong

Conventional reinforcement learning optimizes a model via policy gradient method for a higher reward which is computed from a stable reward function. Most cases, in which have only one factor, are able to train a neural network model as we wish. For the problem caused by more than one factor, we leverage combination of weighted factors to describe its reward function. However, these functions require careful tuning and will backfire if their design assumptions are violated, for example, Section~\ref{sec:motivation}. To tackle this problem, we propose a novel sight to describe the reward function: only to represent the reward as win or loss rather than an actual reward score. As shown in Figure~\ref{fig:overview}, at first we initialize two different agents agent_0 and agent_1. These two agents then train its neural network in the same environment and save their state, action and reward for each step respectively.